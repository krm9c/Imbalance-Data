\section{May 25\\ 
{\large Deriving the Simplest Policy Gradient: Part 2}}


\subsection{The Log-Derivative Trick}

The log-derivative trick is based on a simple rule from calculus: the derivative of $\log x$ with respect to $x$ is $1/x$. When rearranged and combined with chain rule, we get:

$\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta).$

\subsection{The Log-Probability of a Trajectory}\label{log_prob}

The log-derivative trick is a fundamental principle in the derivative rule stating that the derivative of $\log x$ with respect to $x$ is $1/x$. Upon rearranging this rule and applying the chain rule, we obtain the following relation:
$$
\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg).$$
In this formula, $\nabla_{\theta} P(\tau | \theta)$ denotes the gradient of the probability of the trajectory $\tau$ given the policy parameter $\theta$, while $\nabla_{\theta} \log P(\tau | \theta)$ represents the gradient of the log probability of the same trajectory. This trick simplifies the gradient calculation process, which is integral to the policy gradient algorithm.

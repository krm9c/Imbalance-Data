\section{May 24, 2023\\ 
{\large Deriving the Simplest Policy Gradient}}
The Policy Gradient algorithm is centered on a parameterized stochastic policy, denoted as $\pi_{\theta}$. The primary objective is to maximize the expected return, which is represented as
$$J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}}\left[{R(\tau)}\right]. $$
In this equation, $\tau=(s_0, a_0, s_1, a_1, ...)$ is a trajectory, or a sequence of states and actions generated by the current policy $\pi_{\theta}$, and $R(\tau)$ stands for return that we aim to maximize. In our discussion, we specifically deal with the finite-horizon discounted return case where the return is defined as:

$$R(\tau) = \sum_{t=0}^{T} \gamma^t r_t$$
assuming the generated trajectory includes steps $t$ ranging from $0$ to $T$.

Policy optimization is achieved via gradient ascent, expressed as:
$$\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}.$$

In this expression, $\nabla_{\theta} J(\pi_{\theta})$ is the policy gradient that signifies the gradient of the policy performance. The algorithms which utilize this approach for policy optimization are commonly referred to as policy gradient algorithms.

For the implementation of the policy gradient algorithm, we need a computable expression for the policy gradient. This process usually involves the following steps:

\begin{enumerate}
  \item Calculation of the Probability of a Trajectory.
  \item Application of the Log-Derivative Trick.
  \item Computation of the Log-Probability of a Trajectory.
  \item Evaluation of Gradients of Environment Functions.
  \item Calculation of Grad-Log-Prob of a Trajectory.
\end{enumerate}

In the subsequent sections, we will delve deeper into these stages, ultimately arriving at a more sophisticated form commonly used in standard policy gradient algorithm implementations.

\subsection{Probability of a Trajectory}
The trajectory, denoted by $\tau = (s_0, a_0, ..., s_{T+1})$, represents a sequence of states and actions. When actions are derived from the policy $\pi_{\theta}$, the probability of this trajectory can be determined:
$$
P(\tau|\theta) = \rho_0 \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t),$$
where $\rho_0 $  is the start-state distribution.

\subsection{The Log-Derivative Trick}

The log-derivative trick is based on a simple rule from calculus: the derivative of $\log x$ with respect to $x$ is $1/x$. When rearranged and combined with chain rule, we get:

$\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta).$

\subsection{The Log-Probability of a Trajectory}\label{log_prob}

The log-derivative trick is a fundamental principle in the derivative rule stating that the derivative of $\log x$ with respect to $x$ is $1/x$. Upon rearranging this rule and applying the chain rule, we obtain the following relation:
$$
\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg).$$
In this formula, $\nabla_{\theta} P(\tau | \theta)$ denotes the gradient of the probability of the trajectory $\tau$ given the policy parameter $\theta$, while $\nabla_{\theta} \log P(\tau | \theta)$ represents the gradient of the log probability of the same trajectory. This trick simplifies the gradient calculation process, which is integral to the policy gradient algorithm.

\subsection{Gradients of Environment Functions}\label{sec_gradient_env}

The environment's characteristics are independent of the policy parameter $\theta$. As a result, the gradients of the initial state distribution $\rho_0(s_0)$, the transition probability $P(s_{t+1}|s_t, a_t)$, and the return $R(\tau)$ with respect to $\theta$ are all zero. This essentially means that any changes to the policy parameter $\theta$ do not directly influence these quantities.

\textcolor{red}{Not finished yet}


\subsection{Grad-Log-Prob of a Trajectory}


\section{May 24, 2023\\ 
{\large Deriving the Simplest Policy Gradient}}
The Policy Gradient algorithm is centered on a parameterized stochastic policy, denoted as $\pi_{\theta}$. The primary objective is to maximize the expected return, which is represented as
$$J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}}\left[{R(\tau)}\right]. $$
In this equation, $\tau=(s_0, a_0, s_1, a_1, ...)$ is a trajectory, or a sequence of states and actions generated by the current policy $\pi_{\theta}$, and $R(\tau)$ stands for return that we aim to maximize. In our discussion, we specifically deal with the finite-horizon discounted return case where the return is defined as:

$$R(\tau) = \sum_{t=0}^{T} \gamma^t r_t$$
assuming the generated trajectory includes steps $t$ ranging from $0$ to $T$.

Policy optimization is achieved via gradient ascent, expressed as:
$$\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}.$$

In this expression, $\nabla_{\theta} J(\pi_{\theta})$ is the policy gradient that signifies the gradient of the policy performance. The algorithms which utilize this approach for policy optimization are commonly referred to as policy gradient algorithms.

For the implementation of the policy gradient algorithm, we need a computable expression for the policy gradient. This process usually involves the following steps:

\begin{enumerate}
  \item Calculation of the Probability of a Trajectory.
  \item Application of the Log-Derivative Trick.
  \item Computation of the Log-Probability of a Trajectory.
  \item Evaluation of Gradients of Environment Functions.
  \item Calculation of Grad-Log-Prob of a Trajectory.
\end{enumerate}

In the subsequent sections, we will delve deeper into these stages, ultimately arriving at a more sophisticated form commonly used in standard policy gradient algorithm implementations.

\subsection{Probability of a Trajectory}
The trajectory, denoted by $\tau = (s_0, a_0, ..., s_{T+1})$, represents a sequence of states and actions. When actions are derived from the policy $\pi_{\theta}$, the probability of this trajectory can be determined:
$$
P(\tau|\theta) = \rho_0 \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t),$$
where $\rho_0 $  is the start-state distribution.

\subsection{The Log-Derivative Trick} \textcolor{red}{Not finished yet}


\subsection{The Log-Probability of a Trajectory}


\subsection{Gradients of Environment Functions}


\subsection{Grad-Log-Prob of a Trajectory}


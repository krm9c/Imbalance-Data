\section{June 09\\ 
{\large Hyper-parameter tunning tasks}}

Hyper-parameters to tune:


\begin{sidewaystable}[h!]{\small\caption{Summary of hyper parameters that require fine-tuning. The setting of hyperparameter fine-tuning is referenced in \cite{andrychowicz2020matters}.}
\begin{tabular}{cccccc}
\toprule
{\bf Hyper parameter}  & {\bf Description}  & {\bf Feature Type} & {\bf Range}  & {\bf Default}  & {\bf Reference} \\ \midrule
Actor width        & Number of units per layer in the Policy Network           & Integer      & $\left[8, 512\right]$    & 128            & P24 Fig. 18 \\ 
Actor depth        & Number of layers in the Policy Network & Integer      & $\left[1, 8\right]$      & 3             & P25 Fig. 22 \\ 
Actor learn rate   & Learning rate in the Policy optimizer  & Float        & $\left[10^{-2}, 10^{-5}\right]$  & $10^{-3}$ & P41 Fig. 69 \\ \midrule
Critic width       & Number of units per layer in the Value Network            & Integer      & $\left[8, 512\right]$   & 128           & P25 Fig. 21 \\ 
Critic depth       & Number of layers in the Value Network  & Integer      & $\left[1, 8\right]$      & 3             & P25 Fig. 20 \\ 
Critic learn rate  & Learning rate in the Value optimizer   & Float        & $\left[10^{-2}, 10^{-5}\right]$  & $10^{-3}$     & P41 Fig. 69 \\ \midrule
Clipping threshold & Control how much is allowed to change for each update     & Float        & $\left[0.1, 0.5\right]$  & 0.3           & Fig. 10, 32 \\ 
Discount factor    & How much do future rewards contribute to the total return & Float        & $\left[0.8, 0.99\right]$ & 0.99          & P38 Fig. 60 \\ \midrule
Initialization     & Initialization strategies for the neural networks         & Categorical  & 
\begin{tabular}[c]{@{}c@{}}"Glorot normal"\\ "Glorot uniform"\\ "He normal"\\ "He uniform"\\ "LeCun normal"\\ "LeCun uniform"\\ "Orthogonal"\end{tabular} & "Glorot normal"     
&  P26 Fig. 27           \\  \midrule  \midrule 
Activation         & Activation function of each layer of neural network                 & Categorical  & 
\begin{tabular}[c]{@{}c@{}}"ELU"\\ "Tanh"\\ "ReLU"\\ "Swish"\\ "Sigmoid"\\ "Leaky ReLU"\end{tabular}               
& "Tanh"       & P27 Fig. 30 \\ \midrule
Optimizer          & Optimization strategy of neural networks in parameter updating      & Categorical  & 
\begin{tabular}[c]{@{}c@{}}"SGD"\\ "Adam"\\ "RMSProp"\\ "Adagrad"\end{tabular}                                     
& "Adam"       & P41 Fig. 66 \\ \midrule  \midrule
% Optimizer momentum & Accelerate the optimizer towards the minima and reduce oscillations & Float        & $\left[0,0.9\right]$      & 0.9    & P41 Fig. 67 \\ \midrule \midrule
% Rollout            & Number of workers to generate multiple trajectories simultaneously  & Integer      & $\left[1, 100K\right]$ & 18 & None  \\ \midrule \midrule
Regularizer        & A term added to the loss function to prevent overfitting            & Categorical  & & & Figs. 83-91
\\ \bottomrule 
\end{tabular}}
\end{sidewaystable}

\clearpage
\subsection{Week 05 Summary}

\subsubsection{Task 1}
\begin{itemize}
\item Status: (Completed)
\item Make a table of all the hyperparameters to tune with their type, range, and importance.
\item The most important hyperparameters that need to be fine-tuned have been determined.
\item A portion of the second most important hyperparameters that require fine-tuning have been identified.
\item There is still a portion of second most important hyperparameters that needs to be discussed, such as standard deviation lower bound.
\end{itemize}

\subsubsection{Task 2}
\begin{itemize}
\item Status: (Completed)
\item Configure Bebop environment.
\item The environment configuration has been set up.
\item Tested deephyper demo and the PPO demo.
\item Completed the job submission on Bebop.
\end{itemize}

\subsubsection{Task 3}
\begin{itemize}
\item Status: (In Progress)
\item Get familiar with the code.
\item Just start reading the code, no problem yet.
\end{itemize}

\subsubsection{Task 4}
\begin{itemize}
\item Status: (In Progress)
\item Look into MPI program in Deephyper.
\end{itemize}

\subsection{Week 06 Plans}

\subsubsection{Task 1}
\begin{itemize}
\item Get familiar with the code.
\item Incorporate PPO code into DeepHyper framework.
\end{itemize}

\subsubsection{Task 2}
\begin{itemize}
\item Look into multi-objective optimization in Deephyper.
\item Get familiar with MPI program, and try to apply it with PPO code.
\end{itemize}

\subsubsection{Task 3}
\begin{itemize}
\item Set the hyperparameters that need to be fine-tuned for the PPO task in the DeepHyper framework.
\item Make sure that the framework can work normally and explore hyperparameter space.
\end{itemize}

\subsubsection{Task 4}
\begin{itemize}
\item Define the metrics to estimate the performance of PPO training.
\item How to score the model according to the Rate of convergence during model training?
\item Set a performance threshold and measure the number of iterations required to reach that threshold.
\item How to balance the scores from accumulative rewards and from the rate of convergence?
\end{itemize}

\subsection{Week 07 Plans}

\subsubsection{Task 1}
\begin{itemize}
\item Explore the most important hyperparameters and find the best set.
\end{itemize}

\subsubsection{Task 2}
\begin{itemize}
\item Make sure MPI program can be used to speed up the PPO training process.
\end{itemize}

\subsection{Week 08 Plans}
\subsubsection{Task 1}
\begin{itemize}
\item Start to explore the second important hyperparameters.
\end{itemize}